{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cODeBzg1alZC"
   },
   "source": [
    "# Data Science Lab: Lab 1\n",
    "\n",
    "Submit:\n",
    "\n",
    "A pdf of your notebook with solutions.\n",
    "A link to your colab notebook or also upload your .ipynb if not working on colab.\n",
    "\n",
    "**Goals of this Lab:**\n",
    "1.  Review important results from probability, such as the CLT.\n",
    "2.  Connecting that review with basic Python commands.\n",
    "3.  Practice with Pandas, Numpy and Data Exploration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "4Xvcqq7SapU5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jn/rltnfjms6p74n306blbwhhwr0000gn/T/ipykernel_10615/1432850387.py:16: DeprecationWarning: `set_matplotlib_formats` is deprecated since IPython 7.23, directly use `matplotlib_inline.backend_inline.set_matplotlib_formats()`\n",
      "  set_matplotlib_formats('png', 'pdf')\n"
     ]
    }
   ],
   "source": [
    "# Some useful libraries\n",
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "\n",
    "#Pandas for data structure and analysis tools\n",
    "import pandas as pd\n",
    "\n",
    "#seaborn and matplotlib for plotting\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#for nice vector graphics\n",
    "%matplotlib inline\n",
    "\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('png', 'pdf')\n",
    "\n",
    "np.random.seed(42) # Fixed seed for reproducibility, do not change this value\n",
    "rng = default_rng()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2cb_GbUlauEV"
   },
   "source": [
    "# Problem 1\n",
    "\n",
    "**Part 1.** Generate 1,000 samples of 2 dimensional data from the Gaussian distribution $\\left(\\begin{array}{cc}X_{i}\\\\Y_{i}\\end{array}\\right)∼N\\biggl(\\left(\\begin{array}{cc}-5\\\\5\\end{array}\\right),\\left(\\begin{array}{cc}2 & 0.8\\\\0.8 & 3\\end{array}\\right)\\biggr)$.\n",
    "\n",
    "**Part 2.** Plot these points.\n",
    "\n",
    "**Part 3.** Find the Eigenvectors and Eigenvalues of the covariance matrix using np.linalg.eig, or np.linalg.eigh, or something else of your choice.\n",
    "\n",
    "**Part 4.** Now take the 1,000 points you generated in the first part, and use them to estimate the mean and covariance matrix for this multi-dimensional data using elementary numpy commands, i.e., addition, multiplication, division (do not use a command that takes data and returns the mean or standard deviation).\n",
    "\n",
    "*Remark*: If you did this correctly: You should have made a number of observations. (i) The points you plotted should look like an elongated ellipse. (ii) The axis of elongation (the major axis of the ellipse) should be aligned with the eigenvector you computed that has the largest eigenvalue. The minor axis, should be aligned with the other eigenvector you computed. (iii) In the last part, you computed what is called the *empirical covariance* matrix. This should be quite close to the covariance matrix you used to generate the data. If we used more and more points (10,000, 100,000, etc.), then our empirical estimate would look more and more like what we used to generate the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dp-Seh6FNCT7"
   },
   "source": [
    "# Problem 2: Central Limit Theorem\n",
    "\n",
    "Back in EE351K you learned the Law of Large Numbers, and the Central Limit Theorem, among many other things. The Law of Large Numbers says that if $X_i$ are independent and identically distributed (iid) random variables, then $(1/N) \\sum X_i$ converges to $\\mathbb{E}[X]$. That's the law of large numbers.\n",
    "\n",
    "You also learned the Central Limit Theorem. This says that if $X_i$ are zero mean, have variance 1, and are iid, then $(1/\\sqrt{N}) \\sum X_i$ converges to a random variable. Which random variable? A standard (zero mean, unit variance) Gaussian.\n",
    "\n",
    "We're going to check the central limit theorem empirically, as an excuse to do more practice with Python and numpy and basic plotting.\n",
    "\n",
    "Let $X_i$ be an iid Bernoulli random variable with value \\{-1,1\\}. Look at the random variable\n",
    "$Z_n = \\frac{1}{\\sqrt{n}}\\sum X_i$. By taking 1000 samples from $Z_n$, plot its histogram. {\\bf Note:} To generate 1,000 samples from $Z_n$, you need to generate $1,000 \\times n$ samples of $X_i$, since each $Z$ needs $1,000$ $X_i$'s. Now check that for small $n$ (set $n= 5$) $Z_n$ does not look that much like a Gaussian, but when $n$ is bigger (set $n = 50$) it looks much more like a Gaussian. Check also for much bigger $n$: $n = 250$, to see that at this point, one can really see the bell curve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OG1S6pFANq4b"
   },
   "source": [
    "# Problem 3\n",
    "\n",
    "Download from Canvas/Files the datasetPatientData.csv. Each row is a patient and the last column is the condition that the patient has.  Do data exploration using Pandas and other visualization tools to understand what you can about the data set. For example:\n",
    "\n",
    "Part 1.  How many patients and how many features are there?\n",
    "\n",
    "Part 2.  What is the meaning of the first 4 features?  See if you can understand what they mean.\n",
    "\n",
    "Part 3.  Are there missing values?  Replace them with the average of the corresponding feature column and plot the feature histograms with \n",
    "\n",
    "Part 4.  How could you test which features strongly influence the patient condition and which do not? List what you think are the three most important features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A96ER2u0OpaH"
   },
   "source": [
    "# Problem 4\n",
    "\n",
    "The goal of this exercise is for you to get more experience with Pandas, and to get a chance to explore a cool data set.  Download the fileNames.zip from Canvas.  This contains the frequency of all names that appeared more than 5 times on a social security application from 1880 through 2015.\n",
    "\n",
    "Part 1. Write a program that on input $k$ and XXXX, returns the top $k$ names from year XXXX. Print out the top 100 names from the year 2000\n",
    "\n",
    "Part 2. Write a program that on input Name returns the frequency for men and women of the name Name. Plot the frequency of the name \"Alex\" from the year 1880 to 2015\n",
    "\n",
    "Part 3. It could be that names are more diverse now than they were in 1880, so that a name may be relatively the most popular, though its frequency may have been decreasing over the years.  Modify the above to return the relative frequency. Plot the relative frequency of the name \"Alex\" from the year 1880 to 2015\n",
    "\n",
    "Part 4. Find all the names that used to be more popular for one gender, but then became more popular for another gender and print out the first 100 names (alphabetized).\n",
    "\n",
    "•(Optional) Find something cool about this data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "01vCW44LTMC7"
   },
   "source": [
    "# Problem 5\n",
    "\n",
    "We looked at the MNIST data set in class. Recall that MNIST is a data set of handwritten digits. It is considered one of the ``easiest'' image recognition problems in computer vision. You can find the MNIST data set which we will use, here: https://www.openml.org/d/554. Though we haven't introduced decision trees formally, we have had a chance to see them in action in class. This exercise is an opportunity to play around with this data set, and in advance of when we get to talk about decision trees in detail, have a chance to see how they work. In short, this is an exercise in learning-by-doing.\n",
    "\n",
    "\n",
    "Part 1. (Nothing to submit) Make sure you can run through the entire Colab notebook posted. Especially if you haven't used Python, try to understand what every line is doing.\n",
    "\n",
    "Part 2. How many data points are there, how many features are there, and what do the features represent?\n",
    "\n",
    "Part 3. Compute how many times each digit appears in the dataset.\n",
    "\n",
    "Part 4. Read the documentation for sklearn.model_selection.train_test_split and explain what this does.\n",
    "\n",
    "Part 5. Read the documentation for DecisionTreeClassifier, and explain what score means.\n",
    "\n",
    "Part 6. What happens to the **training score** as you increase the depth of the tree? Explain.\n",
    "\n",
    "Part 7. What happens to the difference between **training score** and **testing score** as you increase the depth of the tree? Explain.\n",
    "\n",
    "Part 8. Fix the depth of the three, say, depth=7. Then plot the difference of training score - testing score when you train on: 100, 500, 5000, 10000, 15,0000, 20,000, 25,000 points, always computing testing score by evaluating on the complement of the training set. Plot this trend.  Try to explain what you are seeing.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "naZRl9cbmYbT"
   },
   "source": [
    "# Problem 6\n",
    "\n",
    "We now turn to a somewhat more sophisticated data set: CIFAR10.\n",
    "Here is an initial colab notebook: https://colab.research.google.com/drive/1H3a4yVuZLatBvFjrUp5aFBJn_vfmXj7o?usp=sharing\n",
    "\n",
    "Part 1. How many data points are there, and how many labels? How many points for each label?\n",
    "\n",
    "Part 2. There are two ``TO DOs'' listed in the colab notebook. Complete these.\n",
    "\n",
    "\n",
    "If you did this correctly and ran the notebook, you noticed that CIFAR10 indeed looks like a ``harder'' problem. Deep trees are again doing very well on the training set, and they do a little better than guessing on the testing data, but not as well as they do on MNIST. We will revisit CIFAR10 several times, as we develop more powerful tools. And we will see that we will do much better than deep decision trees!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CYxJU9YInYSW"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
